---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.0
  kernelspec:
    display_name: bioinf
    language: python
    name: bioinf
---

```{python}
# %autosave 0
seed = 1
```

```{python}
from nasbench import api

nasbench_path = '../data/nasbench_only108.tfrecord'
nb = api.NASBench(nasbench_path)
```

```{python}
import glob
import os

train_pretrained = glob.glob('../../../pretrained_nets/hashes/train_hashes_100_splits/out_?/')
train_pretrained += glob.glob('../../../pretrained_nets/hashes/train_hashes_100_splits/out_??/')

val_pretrained = glob.glob('../../../pretrained_nets/hashes/valid_hashes_60_splits/out_?/')
val_pretrained += glob.glob('../../../pretrained_nets/hashes/valid_hashes_60_splits/out_??/')
val_pretrained
```

```{python}
import torch
from info_nas.datasets.arch2vec_dataset import get_labeled_unlabeled_datasets

#torch.backends.cudnn.benchmark = True
device = torch.device('cuda')

# device = None otherwise the dataset is save to the cuda as a whole
labeled, unlabeled = get_labeled_unlabeled_datasets(nb, device=device, seed=seed,
                                                    train_pretrained=train_pretrained,
                                                    valid_pretrained=val_pretrained,
                                                    train_labeled_path='../data/train_long.pt',
                                                    valid_labeled_path='../data/valid_long.pt')
```

```{python}
print(labeled['train'].keys())
labeled['train']['net_hashes']
```

```{python}
from info_nas.datasets.io.transforms import SortByWeights

include_bias = True
sorter = SortByWeights(include_bias=include_bias, fixed_label=None)
```

```{python}
from info_nas.datasets.io.semi_dataset import labeled_network_dataset

train_dataset = labeled_network_dataset(labeled['train'], transforms=sorter,
                                        return_hash=True, return_ref_id=True)
train_dataset.get_batch_names()
```

```{python}
for b in train_dataset:
    print(b.keys())
    break
b['ref_id']
```

```{python}
import numpy as np


outputs = []
labels = []
hashes = []
ref_ids = []

print(len(train_dataset))
for i, b in enumerate(train_dataset):
    if i % 50000 == 0:
        print(i)
    
    outputs.append(b['output'])
    labels.append(b['label'])
    hashes.append(b['hash'])
    ref_ids.append(b['ref_id'])
    
outputs = np.array([o.cpu().numpy() for o in outputs])
labels = np.array(labels)
hashes = np.array(hashes)
ref_ids = np.array(ref_ids)
```

```{python}
# compare

```

```{python}
from info_nas.datasets.io.transforms import SortByWeights

include_bias = True
sorter2 = SortByWeights(include_bias=include_bias, fixed_label=2)
```

```{python}
sorter2.fixed_label
```

```{python}
from info_nas.datasets.io.semi_dataset import labeled_network_dataset

train_dataset2 = labeled_network_dataset(labeled['train'], transforms=sorter2,
                                         return_hash=True, return_ref_id=True)
train_dataset2.get_batch_names()
```

```{python}
import numpy as np


outputs2 = []
labels2 = []
hashes2 = []
ref_ids = []

print(len(train_dataset2))
for i, b in enumerate(train_dataset2):
    if i % 50000 == 0:
        print(i)
    
    outputs2.append(b['output'])
    labels2.append(b['label'])
    hashes2.append(b['hash'])
    
outputs2 = np.array([o.cpu().numpy() for o in outputs2])
labels2 = np.array(labels2)
hashes2 = np.array(hashes2)
```

## Heatmap and stats

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

label = 0

hashmap = hashes == hashes[1]
labelmap = labels == label
#omap = labelmap
omap = np.logical_and(hashmap, labelmap)

letadlo = outputs[omap][:]
lmax = letadlo.max()

lmean = letadlo.mean()
lstd = letadlo.std()

plt.figure()
#sns.heatmap(letadlo / lmax)
sns.heatmap((letadlo - lmean)/ lstd)
plt.tight_layout()
plt.savefig(f"heatmap_all-net_{0}-data-label_{label}.png")
plt.show()
plt.close()
```

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

label = 2

hashmap = hashes2 == hashes2[1]
labelmap = labels2 == label
#omap = labelmap
omap = np.logical_and(hashmap, labelmap)

letadlo = outputs2[omap][:]
lmax = letadlo.max()

plt.figure()
sns.heatmap(letadlo / lmax)
plt.tight_layout()
plt.savefig(f"heatmap_all-net_{0}-data-label_{label}.png")
plt.show()
plt.close()
```

```{python}
#compare plot end
```

```{python}
o = outputs[labels == label]
flato = o[:].flatten()

plt.figure()
sns.distplot(flato[flato > 5])
plt.show()
```

```{python}
iout = norm_out[norm_hash == norm_hash[139]]
jout = norm_out[norm_hash == norm_hash[396]]

np.sum(np.mean((iout - jout) ** 2, axis=1))
```

```{python}
# normalize per net (per label)
label = 1
labelmap = labels == label

norm_out = outputs.copy()

before = False

if before:
    norm_hash = hashes[labelmap]
    norm_out = norm_out[labelmap]
    norm_ref = ref_ids[labelmap]
else:
    norm_hash = hashes
    norm_ref = ref_ids

for i, net_hash in enumerate(np.unique(norm_hash)):
    hmax = norm_out[norm_hash == net_hash].max()
    
    hmean = norm_out[norm_hash == net_hash].mean()
    hstd = norm_out[norm_hash == net_hash].std()
    
    #norm_out[norm_hash == net_hash] /= hmax
    norm_out[norm_hash == net_hash] = (norm_out[norm_hash == net_hash] - hmean) / hstd
    
if not before:
    norm_hash = hashes[labelmap]
    norm_out = norm_out[labelmap]
    norm_ref = ref_ids[labelmap]
```

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(20,20))
sns.heatmap(norm_out[norm_ref == norm_ref[0]])
plt.tight_layout()
plt.savefig(f"heatmap_all-nets_all-data_label-{label}.png")
plt.show()
plt.close()
```

## Error analysis (mean vector)

```{python}
# ted je 1
all_hashes = np.unique(norm_hash)
size = len(all_hashes)

errors = np.zeros((size, size))

for i, ival in enumerate(all_hashes):
    print(i)
    for j, jval in enumerate(all_hashes):
        
        iout = norm_out[norm_hash == ival]
        jout = norm_out[norm_hash == jval]
        
        errors[i,j] = np.sum(np.mean((iout - jout) ** 2, axis=1))

plt.figure(figsize=(20,20))
sns.heatmap(errors)
plt.show()
```

```{python}
ind = np.unravel_index(np.argsort(errors, axis=None), errors.shape)

how_many = 20

skip = 0

nskip = 0
n = 0
for i, j in zip(*ind):    
    if i == j or i > j:
        continue
    
    if nskip > 0:
        nskip = nskip + 1 if nskip < skip else 0
        continue
    else:
        nskip += 1
        
    print('---------')
    print(i, j, errors[i, j])
    
    imetrics = nb.get_metrics_from_hash(norm_hash[i])
    jmetrics = nb.get_metrics_from_hash(norm_hash[j])
    
    print('Adj:')
    print(imetrics[0]['module_adjacency'], '\n', jmetrics[0]['module_adjacency'])
    print('Ops:')
    print(imetrics[0]['module_operations'], '\n', jmetrics[0]['module_operations'])
    print('108 accuracy test, valid:')
    itrain = imetrics[1][108][1]['final_test_accuracy']
    jtrain = jmetrics[1][108][1]['final_test_accuracy']
    ival = imetrics[1][108][1]['final_validation_accuracy']
    jval = jmetrics[1][108][1]['final_validation_accuracy']
    
    print(f"{itrain} {jtrain} | {ival} {jval}")
    
    print('---------')
    
    n += 1
    if n >= how_many:
        break
```

```{python}

```

```{python}
np.mean(norm_out[norm_hash == norm_hash[0]], axis=1).shape
```

```{python}
nb.get_metrics_from_hash(norm_hash[i])[1][108][1]
```

```{python}
# fit normalizer (on original data)
#    - original data has
```
